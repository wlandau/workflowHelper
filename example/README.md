# Run the example

- Ensure that [R](https://www.r-project.org/) and [GNU make](https://www.gnu.org/software/make/) are installed, as well as the [`workflowHelper`](https://github.com/wlandau/workflowHelper) package and its [dependencies](https://github.com/wlandau/workflowHelper/blob/master/DESCRIPTION).
- Run `Makefile.R` in an R session to generate the [Makefile](https://www.gnu.org/software/make/) and its constituent [`remake`](https://github.com/richfitz/remake)/[YAML](http://yaml.org/) files.
- Open a [command line program](http://linuxcommand.org/) such as [Terminal](https://en.wikipedia.org/wiki/Terminal_%28OS_X%29) and point to the [current working directory](http://www.linfo.org/cd.html).
- Enter `make` into the command line to run the full workflow. To distribute the work over multiple parallel process, you can instead type `make -j <n>` where `<n>` is the number of processes.
- Optionally, inspect the final output files `coef.csv` and `mse.pdf`.
- Optionally, clean up the output. Typing `make clean` removes files generated by `make`, and `make reset` removes those files plus the [Makefile](https://www.gnu.org/software/make/) and [YAML](http://yaml.org/) files generated by `Makefile.R`. 
- See the end of this document for more options for `make`.

# Details

Suppose I want to 

1. Generate some datasets.
2. Analyze each dataset with multiple statistical methods (`lm` and `glm`).
3. Compute summary statistics of each analysis of each dataset (model coefficients and mean squared error) and aggregate the summaries together.
4. Generate some tables and figures using those agregated summaries.

The file [code.R]("https://github.com/wlandau/workflowHelper/blob/master/example/code.R") contains functions for producing datasets, running analyses, computing summaries, and generating figures and tables. All I need to do is put these pieces together in a workflow.

## Set up the workflow

First, I list the R scripts with my code and the packages it depends on.

```{r}
library(workflowHelper)
sources = strings(code.R, MASS)
```

The `.r` and `.R` extensions distinguish packages from source files. Also, `strings` converts R expressions into character strings, so I could have simply written `sources = c("code.R", "MASS")`.

Next, I list the commands to generate the datasets.

```{r}
datasets = commands(
  poisson100 = poisson_dataset(n = 100),
  normal100 = normal_dataset(n = 100),
  normal1000 = normal_dataset(n = 1000)
)
```

Be sure to give a name to each command (for example, `poisson_dataset(n = 100)` has the name `poisson100`). The `command` function checks for names and returns a named character vector, so I could have simply written `datasets = c(poisson100 = "poisson_dataset(n = 100)", normal100 = "normal_dataset(n = 100)", normal1000 = "normal_dataset(n = 1000)")`. For 4 replicates of each kind of dataset, assign `datasets = reps(datasets, 4)`.

Similarly, I specify the commands to analyze each dataset.

```{r}
analyses = commands(
  lm = lm_analysis(..DATASET..), # Just apply `lm`
  glm = glm_analysis(..DATASET..) # Modify dataset, then apply `glm`
)
```

The `..DATASET..` wildcard stands for the current dataset being analyzed, which will be an object returned by `poisson_dataset` or `normal_dataset`. When I list the methods of summarizing analyses, there is an additional `..ANALYSIS..` wildcard that similary stands for the appropriate object returned by `lm_analysis` or `glm_analysis`.

```{r}
summaries = commands(
  mse = mse_summary(..DATASET.., ..ANALYSIS..), # mean squared error
  coef = coef_summary(..ANALYSIS..) # model coefficients
)
```

Next, I spedicify how to generate output at the end.

```{r}
output = commands(
  mse.pdf = mse_plot(),
  coef.csv = coef_table()
)
```

Optionally, I can prepend some lines to the overarching [Makefile](https://www.gnu.org/software/make/) for the workflow. In this way, I can configure my workflow for a [Slurm](https://en.wikipedia.org/wiki/Slurm_Workload_Manager) or [PBS](https://en.wikipedia.org/wiki/Portable_Batch_System) cluster or simply add comments.

```{r}
begin = c("# This is my Makefile", "# Variables...")
```

The stages and elements of my workflow are now planned. To put them all together, I use `plan_workflow`, which calls `parallelRemake::write_makefile`.

```{r}
plan_workflow(sources, datasets, analyses, summaries, output, begin)
```

Now, there is a [Makefile](https://www.gnu.org/software/make/) in my current working directory. There are also several hidden [YAML](http://yaml.org/) files in the same directory, all of which are necessary to the [Makefile](https://www.gnu.org/software/make/). To actually run or manage the workflow, just open a [command line program](http://linuxcommand.org/) and enter one of the following.

- `make` runs the full workflow, only building results that are out of date or missing.
- `make -j <n>` is the same as above with the workflow distributed over `<n>` parallel processes. Similarly, you can append `-j <n>` to any of the commands below to activate parallelism.
- `make datasets` just makes the datasets.
- `make analyses` just runs the analyses of all the datasets after ensuring that the datasets are up to date.
- `make summaries` computes individual summaries of each analysis of each dataset.
- `make aggregates` aggregates the summaries together.
- `make output` makes the final output of the workflow after ensuring all the previous results are up to date.
- `make clean` removes the files generated by `make`. If some of your files are produced by side effects, `make clean` might not remove them. In that case, updates to dependencies may not trigger the desired rebuilds, so you should read the next section. 
- `make reset` runs `make clean` and then removes the [Makefile](https://www.gnu.org/software/make/) and all its constituent [YAML](http://yaml.org/) files.

## File management

`workflowHelper` tries to manage files for you. Since it uses [`remake`](https://github.com/richfitz/remake), there is a hidden [`storr`](https://github.com/richfitz/storr) cache in `.remake/objects/`, which you should not touch. Instead, focus on the [RDS](http://www.r-bloggers.com/a-better-way-of-saving-and-loading-objects-in-r/) files inside the `CACHE/` folder. Datasets, analyses, and individual summaries are stored there. In this example, the datasets are 

- `CACHE/poisson100.rds`
- `CACHE/normal100.rds`
- `CACHE/normal1000.rds`

The analyses are

- `CACHE/poisson100_lm.rds`
- `CACHE/poisson100_glm.rds`
- `CACHE/normal100_lm.rds`
- `CACHE/normal100_glm.rds`
- `CACHE/normal1000_lm.rds`
- `CACHE/normal1000_glm.rds`

and the summaries are 

- `CACHE/poisson100_lm_coef.rds`
- `CACHE/poisson100_lm_mse.rds`
- `CACHE/poisson100_glm_coef.rds`
- `CACHE/poisson100_glm_mse.rds`
- `CACHE/normal100_lm_coef.rds`
- `CACHE/normal100_lm_mse.rds`
- `CACHE/normal100_glm_coef.rds`
- `CACHE/normal100_glm_mse.rds`
- `CACHE/normal1000_lm_coef.rds`
- `CACHE/normal1000_lm_mse.rds`
- `CACHE/normal1000_glm_coef.rds`
- `CACHE/normal1000_glm_mse.rds`

The aggregates of the summaries are `mse.rds` and `coef.rds`, and they are stored in the same directory as the [Makefile](https://www.gnu.org/software/make/). `mse.rds` contains a named list of mean squared errors, where the names identify the datasets and analyses to which they pertain. `coef.rds` is similar for model coefficients. The commands in `output` to produce `mse.pdf` and `coef.csv` read in `mse.rds` and `coef.rds`, respectively. However, these RDS files are not listed as arguments to `mse_plot` or `coef_table`. `workflowHelper` already treats all the above 23 RDS files as dependencies for the commands in `output`, and calling `mse_plot("mse.rds")` rather than `mse_plot()` would cause "mse.rds" to be listed as a dependency twice, which would cause [`remake`](https://github.com/richfitz/remake) to throw an error. However, this dependency structure triggers the correct rebuilds for `coef.csv` and `mse.pdf`. If you require dependencies other than these RDS files, write them as arguments to your commands. For example, to make sure forward rebuilds are triggered by changes to  `other_file.csv`, write

```{r}
output = commands(
  mse.pdf = mse_plot("other_file.csv"),
  coef.csv = coef_table("other_file.csv")
)
```

# Warning

The [`remake`](https://github.com/richfitz/remake) package mostly focuses on managing R code in a single R session. As an upshot, editing R code triggers rebuilds, but editing intermediate files by hand only triggers later rebuilds. Thus, all your data and output should be generated by code and then left alone. See the [posted issue](https://github.com/richfitz/remake/issues/87) to see this phenomenon reproduced.

